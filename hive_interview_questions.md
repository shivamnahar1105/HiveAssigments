# Hive Interview Questions

#### 1. What is the definition of Hive? What is the present version of Hive?

-- Hive is an open-source data warehouse system that is built on top of Hadoop. It provides a SQL-like interface to query and analyze large datasets stored in Hadoop's distributed file system (HDFS) or other compatible file systems. Present version is 3.1.3.

#### 2. Is Hive suitable to be used for OLTP systems? Why?

-- OLTP systems are designed for handling a large number of small and frequent transactions, such as bank transactions or e-commerce purchases. In contrast, Hive is designed for OLAP (Online Analytical Processing) systems that deal with larger and less frequent queries.

Hive is optimized for handling complex queries over large datasets and performing batch processing, rather than handling numerous small transactions in real-time. Hive's underlying architecture, including the use of Hadoop Distributed File System (HDFS) and MapReduce, makes it more suitable for processing large volumes of data at once rather than handling individual transactions in real-time.

Therefore, if you need a system to handle OLTP workloads, you should consider other technologies that are specifically designed for that purpose, such as relational databases like MySQL or PostgreSQL.

#### 3. How is HIVE different from RDBMS? Does hive support ACID transactions. If not then give the proper reason.

-- Hive is different from traditional RDBMS (Relational Database Management System) in several ways:

> Data model: RDBMS stores data in a structured manner, using tables with predefined schema, whereas Hive stores data in a semi-structured or unstructured format, such as CSV, JSON or Avro, using a schema-on-read approach.

> Query language: RDBMS uses SQL (Structured Query Language) for querying and manipulating data, whereas Hive uses a SQL-like query language called HiveQL (Hive Query Language), which is optimized for querying large datasets.

> Scalability: RDBMS typically works on a single server, whereas Hive is designed to work with large datasets distributed across a cluster of computers.

Hive does not fully support ACID (Atomicity, Consistency, Isolation, and Durability) transactions. While it does support some ACID-like properties, such as atomicity and durability, it lacks full support for consistency and isolation.

The reason for this is that Hive is built on top of Hadoop, which was originally designed for batch processing and not for transactional workloads. Hadoop's underlying file system, HDFS, is also designed to support large-scale batch processing rather than real-time transaction processing. As a result, Hive's architecture is optimized for efficient processing of large volumes of data rather than transaction processing.

However, there are other technologies that can be used with Hive to provide transactional capabilities, such as Apache HBase or Apache Phoenix. These technologies are designed to work with Hadoop and provide ACID transactions for real-time workloads.

#### 4. Explain the hive architecture and the different components of a Hive architecture?

Hive architecture consists of several components, each performing a specific function:

> Metastore: The Metastore is the central repository that stores metadata about the data stored in Hive. It includes information about the database schema, tables, partitions, columns, and their data types. It also stores the location of the data in the Hadoop Distributed File System (HDFS) or other compatible file systems.

> HiveQL: HiveQL is the SQL-like language used to query the data stored in Hive. It is a high-level language that gets translated into MapReduce jobs that execute on the Hadoop cluster.

> Driver: The Driver is the main component responsible for executing the Hive queries. It receives the query from the user, converts it into MapReduce jobs, and submits them to the Hadoop cluster for execution. It also communicates with the Metastore to get information about the data being queried.

> Compiler: The Compiler is responsible for parsing the HiveQL queries and converting them into an execution plan. The execution plan is then optimized for performance and translated into MapReduce jobs.

> Execution Engine: The Execution Engine is responsible for executing the MapReduce jobs generated by the Compiler. It communicates with the Hadoop cluster to execute the MapReduce jobs and fetches the results back to the Driver.

> Hadoop Cluster: The Hadoop Cluster is the underlying infrastructure that runs the MapReduce jobs generated by Hive. It consists of a cluster of computers that work together to store and process large volumes of data.

> Storage: Hive can store data in several formats, including text, CSV, ORC, Parquet, and Avro. The data is stored in HDFS or other compatible file systems, and the Metastore keeps track of the location of the data.

Overall, the Hive architecture is designed to provide a SQL-like interface for querying large volumes of data stored in Hadoop or other compatible file systems. It uses a high-level language (HiveQL) to convert queries into MapReduce jobs, which are executed on the Hadoop cluster. The Metastore stores metadata about the data, while the Execution Engine executes the MapReduce jobs and fetches the results back to the user.

![image](https://user-images.githubusercontent.com/118034802/234181817-c8391cdb-5107-4ac1-b6e5-47649fffe73b.png)

#### 5. Mention what Hive query processor does? And Mention what are the components of a Hive query processor?

The Hive query processor is responsible for processing the queries submitted to Hive and generating an execution plan for the queries. The query processor is composed of several components, each performing a specific function:

* Parser: The Parser is responsible for parsing the HiveQL queries submitted by the user and converting them into an abstract syntax tree.

* Semantic Analyzer: The Semantic Analyzer checks the syntax of the query and performs semantic analysis, including resolving table and column names, and checking for the correctness of the query syntax.

* Query Optimizer: The Query Optimizer generates an optimized execution plan for the query by considering various factors such as join order, filter order, and partition pruning. The optimizer also chooses the most efficient join algorithm and selects the best parallelism degree for the query.

* Query Planner: The Query Planner generates a logical execution plan for the query, which is a set of instructions for how the query should be executed.

* Execution Engine: The Execution Engine is responsible for executing the logical execution plan generated by the Query Planner. It translates the logical execution plan into a physical execution plan that can be executed on the Hadoop cluster. The Execution Engine is also responsible for scheduling and coordinating the execution of the query.

Overall, the Hive query processor is a complex system that works together to generate an optimized execution plan for the queries submitted by the user. It uses a series of components to parse, analyze, optimize, plan, and execute the queries, taking into consideration various factors such as data location, data format, and query complexity.

#### 6. What are the three different modes in which we can operate Hive?

There are three different modes in which we can operate Hive:

* Local Mode: In Local Mode, Hive runs on a single machine and uses the local file system to store data. This mode is useful for testing and debugging queries on small datasets.

* MapReduce Mode: In MapReduce Mode, Hive uses MapReduce to process data stored in Hadoop Distributed File System (HDFS) or other compatible file systems. This mode is suitable for large datasets that require distributed processing.

* Spark Mode: In Spark Mode, Hive uses Apache Spark as the execution engine to process data stored in HDFS or other compatible file systems. This mode is suitable for large datasets that require distributed processing and can provide better performance than MapReduce Mode.

In addition to these modes, Hive also supports different deployment modes such as Standalone, Pseudo-distributed, and Fully-distributed mode, which determine the number of nodes and the type of installation used for running Hive.

#### 7. Features and Limitations of Hive.
Hive is a data warehousing tool that enables users to perform data analysis using SQL-like queries. It has several features that make it a popular tool for big data analysis, including:

> Features:

* SQL-like Query Language: Hive provides a SQL-like query language, HiveQL, which makes it easy for users familiar with SQL to write and execute queries.

* Scalability: Hive is designed to handle large datasets and can scale to process terabytes or petabytes of data.

* Integration with Hadoop: Hive is tightly integrated with Hadoop and can leverage Hadoop's distributed computing framework to process data in a distributed manner.

* Extensibility: Hive provides a framework for adding custom user-defined functions (UDFs) and SerDes to handle different data formats.

* Schema on Read: Hive enables users to define the structure of data on read rather than on write, which provides flexibility in working with structured and semi-structured data.

* Security: Hive provides several security features, including authentication and authorization, to control access to data.

> Limitations:

* Latency: Hive is not designed for real-time processing and can have high latency, especially when running complex queries on large datasets.

* Limited Support for Transactions: Hive does not support ACID transactions, which can make it challenging to maintain data consistency.

* Limited Support for Updates and Deletes: Hive is optimized for batch processing and does not provide efficient support for updates and deletes of data.

* Lack of Indexing: Hive does not provide indexing for non-primary key columns, which can result in slow query performance on large datasets.

* Limited Support for Complex Data Types: Hive has limited support for complex data types, such as arrays and maps, which can make it challenging to work with semi-structured data.

Overall, Hive is a powerful tool for big data analysis that provides several useful features but also has some limitations that users should be aware of when deciding whether to use it for their data analysis needs.







#### 8. How to create a Database in HIVE?

```
create database database_name;
```

#### 9. How to create a table in HIVE?

```
CREATE TABLE table_name (
  column1 data_type,
  column2 data_type,
  ...
) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
STORED AS textfile;
```

#### 10.What do you mean by describe and describe extended and describe formatted with respect to database and table in Hive ?

* **DESCRIBE**: The DESCRIBE command is used to retrieve basic metadata information about a database or table. When used with a database, it displays the list of tables in the database. When used with a table, it displays the list of columns in the table along with their data types.

* **DESCRIBE EXTENDED**: The DESCRIBE EXTENDED command is used to retrieve additional metadata information about a database or table. When used with a database, it displays the list of tables in the database along with their location in HDFS. When used with a table, it displays the list of columns in the table along with their data types, comments, and other details such as the table type, input format, and output format.

* **DESCRIBE FORMATTED**: The DESCRIBE FORMATTED command is used to retrieve metadata information about a database or table in a formatted manner. It provides a more readable output than the other two variants of the DESCRIBE command. When used with a database, it displays the list of tables in the database along with their location in HDFS in a formatted manner. When used with a table, it displays the list of columns in the table along with their data types, comments, and other details such as the table type, input format, and output format in a formatted manner.

#### 11.How to skip header rows from a table in Hive?

```
skip.header.line.count'='2';
```
#### 12.What is a hive operator? What are the different types of hive operators?

In Hive, operators are special symbols or keywords that are used to perform different operations on data. There are several types of operators in Hive, including:

* Arithmetic Operators: Arithmetic operators are used to perform mathematical operations on numerical data. The supported arithmetic operators in Hive include +, -, *, /, %, and DIV.

* Comparison Operators: Comparison operators are used to compare two values and return a boolean value (true or false) based on the comparison. The supported comparison operators in Hive include =, <> or !=, <, >, <=, and >=.

* Logical Operators: Logical operators are used to combine boolean expressions and return a boolean value based on the result. The supported logical operators in Hive include AND, OR, and NOT.

* Bitwise Operators: Bitwise operators are used to perform bitwise operations on binary data. The supported bitwise operators in Hive include &, |, ^, ~, <<, and >>.

* Assignment Operators: Assignment operators are used to assign a value to a variable. The supported assignment operators in Hive include =, +=, -=, *= and /=.

* Conditional Operators: Conditional operators are used to select one of two values based on a condition. The supported conditional operators in Hive include CASE, WHEN, THEN, ELSE, and END.

* String Operators: String operators are used to perform operations on string data. The supported string operators in Hive include CONCAT, LENGTH, LOWER, UPPER, and SUBSTR.

These are some of the most commonly used operators in Hive, and they can be used in conjunction with Hive queries to perform various operations on data stored in Hive tables.

#### 13.Explain about the Hive Built-In Functions

Hive provides a wide range of built-in functions that can be used to perform various operations on data stored in Hive tables. These functions can be broadly categorized into the following types:

* Mathematical Functions: These functions are used to perform mathematical operations on numerical data. Some of the commonly used mathematical functions in Hive include ABS, CEIL, FLOOR, ROUND, EXP, LOG, POWER, SQRT, RAND, SIGN, and TRUNC.

* String Functions: These functions are used to manipulate and process string data. Some of the commonly used string functions in Hive include CONCAT, LENGTH, LOWER, UPPER, SUBSTR, REPLACE, REGEXP_REPLACE, TRIM, LTRIM, RTRIM, INITCAP, and SPLIT.

* Date/Time Functions: These functions are used to work with date and time data. Some of the commonly used date/time functions in Hive include CURRENT_DATE, CURRENT_TIMESTAMP, DATE_ADD, DATE_SUB, DATEDIFF, MONTHS_BETWEEN, YEAR, MONTH, DAY, HOUR, MINUTE, and SECOND.

* Conditional Functions: These functions are used to apply conditional logic to data. Some of the commonly used conditional functions in Hive include CASE, WHEN, IF, COALESCE, NULLIF, and IFNULL.

* Aggregate Functions: These functions are used to perform aggregate operations on data, such as SUM, AVG, MIN, MAX, and COUNT.

* Collection Functions: These functions are used to work with arrays and maps. Some of the commonly used collection functions in Hive include ARRAY, MAP, SIZE, ELEMENT_AT, and MAP_KEYS.

#### 14. Write hive DDL and DML commands.

> DDL Commands: 

* CREATE TABLE
* DROP TABLE
* ALTER TABLE
* DESCRIBE TABLE
* SHOW TABLES

> DML Commands:

* SELECT
* INSERT INTO
* UPDATE
* DELETE

#### 15.Explain about SORT BY, ORDER BY, DISTRIBUTE BY and CLUSTER BY in Hive.

SORT BY, ORDER BY, DISTRIBUTE BY, and CLUSTER BY are four important clauses in Hive that are used to control the order and distribution of data within a Hive query.

* **SORT BY**: The SORT BY clause is used to sort the data within a single reducer. It sorts the data based on the specified column in ascending or descending order. However, it does not guarantee a total order of the data.

* **ORDER BY**: The ORDER BY clause is used to sort the data across all the reducers. It sorts the data based on the specified column in ascending or descending order and guarantees a total order of the data. However, it requires a shuffle and sort phase that can be time-consuming and resource-intensive. 

* **DISTRIBUTE BY**: The DISTRIBUTE BY clause is used to distribute the data to the reducers based on the specified column. It does not sort the data but ensures that all the records with the same value for the specified column are sent to the same reducer.

* **CLUSTER BY**: The CLUSTER BY clause is similar to the DISTRIBUTE BY clause, but it also sorts the data based on the specified column within each reducer. This can improve query performance by reducing the amount of data that needs to be processed by each reducer. 

#### 16.Difference between "Internal Table" and "External Table" and Mention when to choose “Internal Table” and “External Table” in Hive?

In Hive, there are two types of tables: Internal and External.

* **Internal Table**: An internal table is a managed table, where Hive is responsible for managing both the metadata and the data itself. When an internal table is created, Hive creates a directory in the warehouse directory to store the data. Dropping an internal table will remove both the metadata and the data stored in the warehouse directory. Internal tables are suitable for use cases where the data is generated by Hive and is not expected to be accessed or modified outside of Hive.

* **External Table**: An external table is a table where the data is not managed by Hive, but is stored outside of Hive, in an HDFS directory or an external storage system. When an external table is created, Hive creates only the metadata, but the data remains outside of Hive's control. Dropping an external table will only remove the metadata, leaving the data in the external storage system untouched. External tables are suitable for use cases where the data is generated outside of Hive or needs to be accessed by other applications.

> When to Choose Internal vs. External Tables:

Use Internal Tables when the data is generated by Hive and is not expected to be accessed or modified outside of Hive. Also, use internal tables when you want Hive to manage both the metadata and the data.
Use External Tables when the data is generated outside of Hive or needs to be accessed by other applications. Also, use external tables when you want to share the same data between different systems or want to keep the data even if the table is dropped.

#### 17.Where does the data of a Hive table get stored?

In Hive, the location of the data of a table depends on whether the table is an internal table or an external table.

* Internal Table: For an internal table, Hive is responsible for managing both the metadata and the data itself. When an internal table is created, Hive creates a directory in the warehouse directory to store the data. The default location of the warehouse directory is **/user/hive/warehouse** in HDFS, but it can be changed in the hive-site.xml configuration file.

* External Table: For an external table, the data is not managed by Hive, but is stored outside of Hive, in an HDFS directory or an external storage system. When an external table is created, Hive creates only the metadata, but the data remains outside of Hive's control. The location of the data for an external table is specified using the LOCATION clause when the table is created.

In both cases, the data of a Hive table is stored in HDFS or an external storage system.

#### 18.Is it possible to change the default location of a managed table?

Yes, it is possible to change the default location of a managed table in Hive.

When a managed table is created, Hive creates a directory in the warehouse directory to store the data, and the default location of the warehouse directory is /user/hive/warehouse in HDFS. However, the default location can be changed in the hive-site.xml configuration file.

> To change the default location of the warehouse directory, follow these steps:

* Open the hive-site.xml configuration file in a text editor.
* Locate the property hive.metastore.warehouse.dir and change its value to the new directory path where you want to store the data.
* Save the changes to the hive-site.xml configuration file and restart the Hive service for the changes to take effect.
After changing the default location of the warehouse directory, any new managed tables created in Hive will use the new location for storing the data. However, existing managed tables will still use the old location, and you will need to move the data manually to the new location and update the table location in Hive.

#### 19.What is a metastore in Hive? What is the default database provided by Apache Hive for metastore?

In Hive, a metastore is a central repository that stores metadata information about tables, partitions, columns, and other objects in the Hive warehouse. The metastore serves as a catalog or a schema for Hive, and it is used by the query processor to translate HiveQL statements into physical execution plans.

The metastore can be configured to use different types of databases, such as MySQL, PostgreSQL, Oracle, or Derby, to store the metadata. By default, Apache Hive provides the Derby database as the embedded metastore database, which is suitable for small-scale deployments or testing purposes.

The default database name provided by Apache Hive for the metastore is "default". When a new table is created without specifying a database name, it is created in the default database. However, you can create multiple databases in the metastore and create tables in different databases as needed. The databases in the metastore are isolated from each other, and each database has its own set of tables and metadata information. You can switch between databases using the USE statement in HiveQL.

#### 20. 20.Why does Hive not store metadata information in HDFS?

Hive does not store metadata information in HDFS because HDFS is designed to store large files and is optimized for sequential access, not for random reads and writes. Metadata information, such as table schemas, column names, and partition information, is typically small in size but frequently accessed by the Hive query processor. Storing metadata in HDFS would require frequent small reads and writes, which can be inefficient and slow down the query performance.

Instead of storing metadata in HDFS, Hive uses a separate database or a metastore to store the metadata information. The metastore serves as a central repository for metadata and is optimized for quick access and retrieval of metadata. The metastore can be configured to use different types of databases, such as MySQL, PostgreSQL, Oracle, or Derby, depending on the size and complexity of the Hive deployment.

Separating metadata from data storage also has other advantages, such as allowing for easier backup and recovery of metadata, and enabling metadata sharing and integration with other tools and systems.

#### 21.What is a partition in Hive? And Why do we perform partitioning in Hive?

In Hive, a partition is a way to divide a table into smaller, more manageable parts based on the values of one or more columns. Each partition is stored as a separate directory or subdirectory in HDFS, and contains a subset of the table's data that matches the partitioning criteria.

Partitioning is a powerful feature in Hive that can improve query performance, reduce storage costs, and simplify data management. By partitioning a table, you can restrict the amount of data that needs to be processed for a given query, which can significantly reduce query latency and improve query throughput. For example, if you partition a large sales transaction table by date, you can easily query for sales data within a specific date range, without scanning the entire table.

Partitioning can also help reduce storage costs by allowing you to store only the data that is needed for a given query. Instead of storing the entire table in a single directory, you can store only the relevant partitions and eliminate the need to store redundant data. This can save a significant amount of storage space, especially for large tables.

Finally, partitioning can simplify data management by allowing you to manage data at a more granular level. For example, you can easily add, delete, or modify partitions without affecting the rest of the table, and you can use different storage formats, compression schemes, or indexing strategies for different partitions. This can help improve data organization and accessibility, and enable more flexible and efficient data processing workflows.




















